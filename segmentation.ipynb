{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Data Simulation/Loading ---\n",
    "# In a real project, replace this with loading your own data.\n",
    "# E.g., df = pd.read_csv('your_customer_data.csv')\n",
    "\n",
    "def generate_synthetic_customer_data(num_customers=40000):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset for customer segmentation.\n",
    "    Data is designed to have distinct clusters based on spending behavior.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create distinct customer segments\n",
    "    # Segment 1: High Spenders, High Frequency\n",
    "    high_spenders = pd.DataFrame({\n",
    "        'customer_id': range(10000),\n",
    "        'total_spent': np.random.normal(5000, 1000, 10000),\n",
    "        'transactions': np.random.normal(50, 10, 10000),\n",
    "        'product_category_preference': np.random.choice(['Electronics', 'Luxury', 'Fashion'], 10000, p=[0.5, 0.3, 0.2])\n",
    "    })\n",
    "    \n",
    "    # Segment 2: Low Spenders, High Frequency\n",
    "    low_spenders_freq = pd.DataFrame({\n",
    "        'customer_id': range(10000, 20000),\n",
    "        'total_spent': np.random.normal(500, 100, 10000),\n",
    "        'transactions': np.random.normal(60, 15, 10000),\n",
    "        'product_category_preference': np.random.choice(['Groceries', 'Home Goods', 'Beauty'], 10000, p=[0.6, 0.2, 0.2])\n",
    "    })\n",
    "    \n",
    "    # Segment 3: Low Spenders, Low Frequency\n",
    "    low_spenders_low_freq = pd.DataFrame({\n",
    "        'customer_id': range(20000, 30000),\n",
    "        'total_spent': np.random.normal(300, 50, 10000),\n",
    "        'transactions': np.random.normal(5, 2, 10000),\n",
    "        'product_category_preference': np.random.choice(['Books', 'Movies', 'Sports'], 10000, p=[0.4, 0.4, 0.2])\n",
    "    })\n",
    "    \n",
    "    # Segment 4: Medium Spenders, Medium Frequency\n",
    "    medium_spenders = pd.DataFrame({\n",
    "        'customer_id': range(30000, 40000),\n",
    "        'total_spent': np.random.normal(1500, 300, 10000),\n",
    "        'transactions': np.random.normal(25, 5, 10000),\n",
    "        'product_category_preference': np.random.choice(['Fashion', 'Home Goods', 'Electronics'], 10000, p=[0.4, 0.4, 0.2])\n",
    "    })\n",
    "    \n",
    "    # Combine all segments\n",
    "    df = pd.concat([high_spenders, low_spenders_freq, low_spenders_low_freq, medium_spenders], ignore_index=True)\n",
    "    df['total_spent'] = df['total_spent'].clip(lower=0)\n",
    "    df['transactions'] = df['transactions'].clip(lower=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = generate_synthetic_customer_data()\n",
    "print(\"Synthetic Customer Data Head:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame Shape:\", df.shape)\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Derives key behavioral features for clustering.\n",
    "    \"\"\"\n",
    "    # Create new features (e.g., average transaction value)\n",
    "    df['avg_transaction_value'] = df['total_spent'] / df['transactions']\n",
    "    \n",
    "    # Select the features for clustering\n",
    "    features = ['total_spent', 'transactions', 'avg_transaction_value']\n",
    "    X = df[features]\n",
    "    \n",
    "    return X, features\n",
    "\n",
    "X, features = feature_engineering(df.copy())\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\nFeatures for Clustering (after scaling):\")\n",
    "print(pd.DataFrame(X_scaled, columns=features).head())\n",
    "\n",
    "# --- 3. Customer Segmentation ---\n",
    "\n",
    "# Determine the optimal number of clusters (e.g., using the Elbow Method)\n",
    "def find_optimal_clusters(data, max_k=10):\n",
    "    inertia = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_k + 1), inertia, marker='o')\n",
    "    plt.title('Elbow Method for Optimal K')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nFinding Optimal K for K-Means (Elbow Method):\")\n",
    "find_optimal_clusters(X_scaled)\n",
    "# Based on the plot, we'll choose K=4 as the clusters are clearly separated in the synthetic data.\n",
    "optimal_k = 4\n",
    "\n",
    "# a) K-Means Clustering\n",
    "print(\"\\n--- Performing K-Means Clustering ---\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df['kmeans_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# b) Gaussian Mixture Model (GMM)\n",
    "print(\"\\n--- Performing Gaussian Mixture Model Clustering ---\")\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
    "df['gmm_cluster'] = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Analyze and visualize the clusters\n",
    "print(\"\\nK-Means Cluster Sizes:\")\n",
    "print(df['kmeans_cluster'].value_counts().sort_index())\n",
    "print(\"\\nGMM Cluster Sizes:\")\n",
    "print(df['gmm_cluster'].value_counts().sort_index())\n",
    "\n",
    "# Visualize the clusters\n",
    "def visualize_clusters(df, x_feature, y_feature, cluster_col, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x=x_feature, y=y_feature, hue=cluster_col, data=df, palette='viridis', legend='full', s=50, alpha=0.7)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(x_feature.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(y_feature.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualizing K-Means Clusters:\")\n",
    "visualize_clusters(df, 'total_spent', 'transactions', 'kmeans_cluster', 'K-Means Customer Segments')\n",
    "\n",
    "print(\"\\nVisualizing GMM Clusters:\")\n",
    "visualize_clusters(df, 'total_spent', 'transactions', 'gmm_cluster', 'GMM Customer Segments')\n",
    "\n",
    "# Characterize the clusters\n",
    "def characterize_clusters(df, cluster_col):\n",
    "    cluster_profiles = df.groupby(cluster_col)[features].mean().reset_index()\n",
    "    return cluster_profiles\n",
    "\n",
    "print(\"\\nK-Means Cluster Profiles (Mean of features):\")\n",
    "print(characterize_clusters(df, 'kmeans_cluster'))\n",
    "\n",
    "print(\"\\nGMM Cluster Profiles (Mean of features):\")\n",
    "print(characterize_clusters(df, 'gmm_cluster'))\n",
    "\n",
    "# --- 4. Recommendation Layer & Actionable Insights ---\n",
    "\n",
    "def generate_recommendations(df, cluster_col):\n",
    "    \"\"\"\n",
    "    Generates product recommendations based on cluster behavior.\n",
    "    In a real-world scenario, this would use a product database.\n",
    "    Here, we'll use the 'product_category_preference' column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Recommendations based on {cluster_col} ---\")\n",
    "    \n",
    "    # Find the most popular product category within each cluster\n",
    "    cluster_recommendations = df.groupby(cluster_col)['product_category_preference'].agg(lambda x: x.mode()[0]).to_dict()\n",
    "    \n",
    "    # Store cluster profiles for actionable insights\n",
    "    cluster_profiles_df = characterize_clusters(df, cluster_col)\n",
    "    \n",
    "    recommendation_insights = {}\n",
    "    for cluster_id, recommendation in cluster_recommendations.items():\n",
    "        profile = cluster_profiles_df[cluster_profiles_df[cluster_col] == cluster_id].iloc[0]\n",
    "        \n",
    "        # Actionable insight text generation\n",
    "        insight = (\n",
    "            f\"Cluster {cluster_id}: These customers spend an average of ${profile['total_spent']:.2f} across \"\n",
    "            f\"{profile['transactions']:.1f} transactions. Their average transaction value is ${profile['avg_transaction_value']:.2f}. \"\n",
    "            f\"**Recommendation**: Target this group with promotions for '{recommendation}'. This will boost cross-sell.\"\n",
    "        )\n",
    "        recommendation_insights[cluster_id] = insight\n",
    "    \n",
    "    return recommendation_insights\n",
    "\n",
    "# Generate recommendations for K-Means clusters\n",
    "kmeans_insights = generate_recommendations(df, 'kmeans_cluster')\n",
    "for cluster, insight in kmeans_insights.items():\n",
    "    print(f\"\\n{insight}\")\n",
    "\n",
    "# Generate recommendations for GMM clusters\n",
    "gmm_insights = generate_recommendations(df, 'gmm_cluster')\n",
    "for cluster, insight in gmm_insights.items():\n",
    "    print(f\"\\n{insight}\")\n",
    "\n",
    "# Simulating the impact of the recommendation system\n",
    "# This is a conceptual example of how \"cross-sell by 20%\" would be measured.\n",
    "# In a real scenario, this would be a A/B test.\n",
    "# Here, we'll just show the concept.\n",
    "def calculate_cross_sell_boost(df):\n",
    "    \"\"\"\n",
    "    Simulates cross-sell boost by assuming a certain percentage of customers\n",
    "    in a cluster would buy a recommended product.\n",
    "    \"\"\"\n",
    "    # Baseline: Assume 1% of customers buy a cross-sell product without recommendations\n",
    "    baseline_cross_sell = 0.01 * len(df)\n",
    "    \n",
    "    # With recommendations: Assume the boost is achieved on a portion of the customer base\n",
    "    # Let's say we target 50% of our customers with the recommendation.\n",
    "    # And the recommendation success rate is 20% higher than the baseline.\n",
    "    customers_targeted = 0.5 * len(df)\n",
    "    boosted_success_rate = 0.01 * 1.2 # 20% boost\n",
    "    \n",
    "    boosted_cross_sell = customers_targeted * boosted_success_rate\n",
    "    \n",
    "    total_new_sales = boosted_cross_sell - baseline_cross_sell\n",
    "    \n",
    "    print(f\"\\n--- Simulated Impact of Recommendation System ---\")\n",
    "    print(f\"Total customers: {len(df):,}\")\n",
    "    print(f\"Simulated new cross-sells due to recommendations: {boosted_cross_sell:.0f} units.\")\n",
    "\n",
    "calculate_cross_sell_boost(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2278dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10db6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a001ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a09c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
